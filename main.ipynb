{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\"\"\"Multinomial Bayesian Classification Project\n",
    "Authors:\n",
    "Nathaniel Champion\n",
    "Hugh Hamilton\n",
    "Chris Beatrez\n",
    "Andrew Kivrak\n",
    "Description:\n",
    "This program takes a JSON file and runs a bayesian classification calculation\n",
    "on the data to algorithmically determine what classification each article would\n",
    "be based on the title and short description provided. It will then compare what\n",
    "the algorithm says is most likely and what the description actually is to\n",
    "determine how well the calculation performed.\"\"\"\n",
    "# Import libraries required for code to run\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# These are the categories the dataset can be classified into. In global scope, not subject to change\n",
    "CATEGORIES = ('CRIME', 'ENTERTAINMENT', 'WORLD NEWS', 'IMPACT', 'POLITICS', 'WEIRD NEWS',\n",
    "              'BLACK VOICES', 'WOMEN', 'COMEDY', 'QUEER VOICES', 'SPORTS', 'BUSINESS',\n",
    "              'TRAVEL', 'MEDIA', 'TECH', 'RELIGION', 'SCIENCE', 'LATINO VOICES',\n",
    "              'EDUCATION', 'COLLEGE', 'PARENTS', 'STYLE', 'GREEN', 'TASTE',\n",
    "              'HEALTHY LIVING', 'WORLDPOST', 'GOOD NEWS', 'FIFTY', 'ARTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################FUNCTIONS######################\n",
    "\n",
    "\"\"\"This function cleans and tokenizes the raw data from the JSON file\"\"\"\n",
    "def createDataSet(dataSet):\n",
    "    \"\"\"These for loops strip out the parts of the JSON data that are not the headline or short article\n",
    "    descriptions, remove all non alphabetical (or space) characters, tokenize all words as list entries\n",
    "    and then reference them to a list of stop words expanded for non-apostrophized English.\"\"\"\n",
    "    allCleanTokens = [] #The list of all clean word tokens to be produced\n",
    "    #We expand the number of stop words to include variations missing apostrophes\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokenSW = nltk.corpus.stopwords.words(\"english\")\n",
    "    for i in stop_words:\n",
    "        s = re.sub(r'[^A-Za-z ]+', '', i)\n",
    "        tokenSW.append(s)\n",
    "    #We now further expand the set of stop words using the extensive list from github user igorbrigadir:\n",
    "    #https://github.com/igorbrigadir/stopwords/tree/master/en\n",
    "    textFiles = Path('./en').glob('*.txt') #use of the glob package to create a list of stop words from .txt\n",
    "    for file in textFiles:\n",
    "        stop_words.append(file.read_text())\n",
    "\n",
    "    for row in dataSet:\n",
    "        dataString = str(row).lower() #Cast all JSON table rows to lowercase string\n",
    "        #Remove all of the string prior to the headline, including article category:\n",
    "        splitString = dataString.split(\"'headline':\")\n",
    "        dataString = str(splitString[1])\n",
    "        #Split along authors and append headline substring to headline list\n",
    "        splitString = dataString.split(\"'authors'\")\n",
    "        #Non alphabetical or space characters removed, and then all words added to wordTokens list\n",
    "        alphaString = splitString[0].replace(\"-\", \" \") #Also replace all dashes with spaces\n",
    "        alphaString = re.sub(\"[^a-zA-Z ]+\", \"\", alphaString)\n",
    "        wordTokens = word_tokenize(alphaString)\n",
    "        #Remove the string portion before the short description substring\n",
    "        splitString = dataString.split(\"'short_description':\")\n",
    "        dataString = str(splitString[1])\n",
    "        #Split to isolate the short description substring from the \"date:\" substring\n",
    "        splitString = dataString.split(\"'date':\")\n",
    "        #Now reallocate alphabetical string with this string subsection, and add to token list\n",
    "        alphaString = splitString[0].replace(\"-\", \" \")\n",
    "        alphaString = re.sub(\"[^a-zA-Z ]+\", \"\", alphaString)\n",
    "        wordTokens += word_tokenize(alphaString)\n",
    "        cleanTokens = [token for token in wordTokens if not token in tokenSW]\n",
    "        allCleanTokens.append(cleanTokens)\n",
    "    checkList = []\n",
    "    for i in allCleanTokens:\n",
    "        freqDistro = nltk.FreqDist(i)\n",
    "        checkList.append(list(freqDistro.most_common()))\n",
    "    #print(checkList)\n",
    "    return checkList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function takes a type denoting the  and a Category list and combines\n",
    "all the word counts. For example if there are two articles that mention\n",
    "'murder', then it will update to 'murder', 2 in the array.\"\"\"\n",
    "\n",
    "def CatWords(Cat, dataSet):\n",
    "    # Creating necessary variables\n",
    "    testWordList = []\n",
    "    counter_1 = 0\n",
    "    WordVar = []\n",
    "    finalWordVar = []\n",
    "    primeArray = []\n",
    "    # For loop to iterate through the articles we are calculating\n",
    "    for x in dataSet:\n",
    "        \"\"\" If the article is equal to the compared category it\n",
    "        appends it to the testing list element that corresponds\n",
    "        to that category. Once it compares all the articles to \n",
    "        a category the counter goes up and will then compare it\n",
    "        to the next category and add the value to that categories\n",
    "        element in listTest array.\n",
    "        \"\"\"\n",
    "        if Cat in str(x):\n",
    "            primeArray.append(listTest[counter_1])\n",
    "        counter_1 += 1\n",
    "    counter_1 = 0\n",
    "    \"\"\" Now that we have an array that has all the words for each \n",
    "    category we compare them to stopWords to remove unimportant words\n",
    "    to reduce the number of comparison and improve the accuracy. If the\n",
    "    word appears a second time in a different article instead of adding\n",
    "    the word it will increment the number of occurrences instead.\n",
    "    \"\"\"\n",
    "    for i in primeArray:\n",
    "        for x in i:\n",
    "            if x[0] not in stopWords:\n",
    "                print(\"flag\")\n",
    "                if x[0] not in testWordList:\n",
    "                    testWordList.append(x[0])\n",
    "                    WordVar.append([x[0], 1])\n",
    "                else:\n",
    "                    index = testWordList.index(x[0])\n",
    "                    value_1 = WordVar[index][1]\n",
    "                    WordVar[index] = ([x[0], (value_1 + 1)])\n",
    "        counter_1 += 1\n",
    "    for i in WordVar:\n",
    "        if i[1] != 1:\n",
    "            finalWordVar.append(i)\n",
    "    # Returns the updated array\n",
    "    return finalWordVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Counts the total number of words in the list and returns\n",
    "with the total number for each category.\"\"\"\n",
    "\n",
    "def CatWordsCount(WordVar):\n",
    "    count = 0\n",
    "    for y in WordVar:\n",
    "        count += y[1]\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function that takes in the array with the list of words\n",
    "and a total word count. It then compares the words in the \n",
    "specific article we are comparing to the list of words in \n",
    "each category and produces a % for the likelihood of it \n",
    "being that article based on the number of time the word \n",
    "appeared in that category and the total number of words.\"\"\"\n",
    "\n",
    "def compCat(wordVar, wordCount):\n",
    "    # Creates variable required for function\n",
    "    finalResults = []\n",
    "    successCounter = 0\n",
    "    \"\"\"listCheck is the array that contains the keyword for \n",
    "    each article in a element within that array\"\"\"\n",
    "    for i in listCheck:\n",
    "        tempWord = \"\"\n",
    "        tempResults = []\n",
    "        counter_2 = 0\n",
    "        tempCheckList = []\n",
    "        \"\"\"the variable i is an array of tuples ex: [('word', 1), \n",
    "        ('word2', 1)] z would then just be a single tuple ('word', 1)\n",
    "         and then z[0] would just be the 'word' which is appended in the \n",
    "         tempCheckList variable.\"\"\"\n",
    "        for z in i:\n",
    "            tempCheckList.append(z[0])\n",
    "            \"\"\"Takes value in the above list and compares it to each value\n",
    "            in the list of words in each category. If it finds the word it\n",
    "            adds a tuple that includes the word and the probability. If it\n",
    "            does not find the word in that list it then provides the probability\n",
    "            of the word appearing a single time. This is our smoothing code\"\"\"\n",
    "            for x in wordVar:\n",
    "                tempWord = tempCheckList[counter_2]\n",
    "                #print(\"article word\" + str(tempWord))\n",
    "                #print(\"category word\" + str(x[0]))\n",
    "                if tempWord == x[0]:\n",
    "                    tempResults.append([x[0], (x[1] / wordCount)])\n",
    "                    successCounter = 1\n",
    "            if successCounter == 0 & wordCount != 0:\n",
    "                tempResults.append([tempWord, (1 / wordCount)])\n",
    "            successCounter = 0\n",
    "            counter_2 += 1\n",
    "        finalResults.append(tempResults)\n",
    "        # returns array with the probability for each word in an article\n",
    "    return finalResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function that determines the probability of each category.\n",
    "ex: if there is 1 crime article and 10 total articles then\n",
    "the probability of it being a crime article is 10%.\"\"\"\n",
    "\n",
    "def probCat(Cat):\n",
    "    probCategory = []\n",
    "    for i in Cat:\n",
    "        count = 0\n",
    "        for x in testSet:\n",
    "            tempData = str(x)\n",
    "            if i in tempData:\n",
    "                count += 1\n",
    "        probCategory.append(count / len(testSet))\n",
    "    return probCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning of program execution: \n",
    "# Importing in the JSON file that contains the data\n",
    "data = [json.loads(line) for line in open('News_Category_Dataset_v2.json', 'r')]\n",
    "\n",
    "\"\"\"These variables are use to cut the data set into 2 parts. The test data set is 80%\n",
    "of the total dataset and will be used as reference. The checkset data set will be\n",
    "20% of the original dataset and will have the bayesian calculation run on it to\n",
    "determine each classification. The for loop will append each data element into a\n",
    "list.\"\"\"\n",
    "#testCount = 100 #commented out lower value of testCount works better in practice\n",
    "#full data set has not been tractable with cubic scaling. \n",
    "testCount = round(.8 * len(data))\n",
    "testSet = []\n",
    "checkSet = []\n",
    "\n",
    "for i in data[0:testCount]:\n",
    "    testSet.append(i)\n",
    "\n",
    "for i in data[testCount + 1: ]:\n",
    "    checkSet.append(i)\n",
    "\n",
    "stopWords = []\n",
    "listTest = createDataSet(testSet)\n",
    "listCheck = createDataSet(checkSet)\n",
    "#\n",
    "AllCatWords = []\n",
    "\"\"\" For loop that uses the CatWords function to create a single\n",
    "array that includes all the words from each category separated into\n",
    "different elements. Next, a second for loop does the same with\n",
    "the CatWordsCount function.\"\"\"\n",
    "for i in CATEGORIES:\n",
    "    tempResults_1 = CatWords(i, testSet)\n",
    "    AllCatWords.append(tempResults_1)\n",
    "AllCatWordCounts = []\n",
    "for i in AllCatWords:\n",
    "    AllCatWordCounts.append(CatWordsCount(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for loop that takes the above arrays and creates a new array that has the\n",
    "probability of the words being in each category. \"\"\"\n",
    "AllCatResults = []\n",
    "for i in CATEGORIES:\n",
    "    localIndex = CATEGORIES.index(i)\n",
    "    tempResults_2 = compCat(AllCatWords[localIndex], AllCatWordCounts[localIndex])\n",
    "    AllCatResults.append(tempResults_2)\n",
    "# Array that uses the probCat function to determine prob of article\n",
    "probArticleType = probCat(CATEGORIES)\n",
    "\n",
    "\"\"\" For loop that does the calculation for determining the likelihood\n",
    "of each article falling into each separate category. Ex: if there are\n",
    "3 keywords it will find the probability of the article being in the\n",
    "first category and multiple that by the probability of each of those\n",
    "keywords being in that category. It then stores that info into an\n",
    "element for that array.\"\"\"\n",
    "testCounter = 0\n",
    "total = 0\n",
    "resultsRecord = []\n",
    "res = []\n",
    "catRes = []\n",
    "for i in AllCatResults:\n",
    "    res.clear()\n",
    "    for z in i:\n",
    "        calcValue = probArticleType[testCounter]\n",
    "        for y in z:\n",
    "            calcValue = calcValue * y[1]\n",
    "        res.append(calcValue)\n",
    "    catRes.append(res[:])\n",
    "    testCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The above for loop saves probability data in this format:\n",
    "[(cat1_art1prob), (cat1_art2prob), (cat1_art3prob)]\n",
    "[(cat2_art1prob), (cat2_art2prob), (cat2_art3prob)]\n",
    "this loop rearranges the list to be the following:\n",
    "[(cat1_art1prob), (cat2_art1prob), (cat3_art1prob)]\n",
    "[(cat1_art2prob), (cat2_art2prob), (cat3_art2prob)]\n",
    "this allows us to compare the probabilities of a each\n",
    "category for a single article.\"\"\"\n",
    "counter = 0\n",
    "tempRes = []\n",
    "wordRes = []\n",
    "counter_3 = 0\n",
    "for x in range(len(catRes[0])):\n",
    "    for i in catRes:\n",
    "        tempValue = i[counter_3]\n",
    "        tempRes.append(tempValue)\n",
    "    wordRes.append(tempRes[:])\n",
    "    tempRes.clear()\n",
    "    counter_3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For loop that determines the max value in each element\n",
    "this max value is the most likely category the article fall in\n",
    "to based on the bayesian calculations done above\"\"\"\n",
    "finalResult = []\n",
    "for i in wordRes:\n",
    "    finalResult.append([i.index(max(i)), max(i)])\n",
    "\n",
    "\"\"\"Compares our answer using Bayesian formula with the correct\n",
    "answer provided by the original JSON file.\"\"\"\n",
    "TotalTotal = 0\n",
    "finalArray = [0]*29\n",
    "totalFinalArray = [0]*29\n",
    "\n",
    "for i in finalResult:\n",
    "    totalFinalArray[i[0]] += 1\n",
    "    if CATEGORIES[i[0]] in str(checkSet[counter]):\n",
    "        TotalTotal += 1\n",
    "        finalArray[i[0]] += 1\n",
    "    counter += 1\n",
    "print(finalArray)\n",
    "print(totalFinalArray)\n",
    "counter = 0\n",
    "for i in finalArray:\n",
    "    accuracyRound = 0\n",
    "    if totalFinalArray[counter] != 0:\n",
    "        accuracy = finalArray[counter]/totalFinalArray[counter]\n",
    "        accuracyRound = round(accuracy, 2)*100\n",
    "    print(\"Cat: \" + str(CATEGORIES[counter]) + \" \" + str(accuracyRound) + \"%\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TotalTotal)\n",
    "print(len(checkSet))\n",
    "\n",
    "totalAccuracy = TotalTotal/len(checkSet)*100\n",
    "totalAccuracyRound = round(totalAccuracy, 2)\n",
    "print(\"total accuracy: \" + str(totalAccuracyRound))\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "print(f'Time taken to run: {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22eefac278ff05bae5abf3a39fa21d59705a9b6cac584246c0ee5695ddf0c981"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
